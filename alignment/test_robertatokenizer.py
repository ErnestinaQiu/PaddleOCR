from paddlenlp.transformers.roberta.tokenizer import RobertaTokenizer
from datasets import load_dataset


data_dir = './data/Pseudo-Latext-ZhEn'
rec_char_dict_path = 'D:/study/dl/MixTex/model'

data = load_dataset(data_dir)
tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path=rec_char_dict_path)

target_text = data['train'][0]['text']
target = tokenizer(target_text, padding="max_length", max_length=256, truncation=True).input_ids
labels = [label if label != tokenizer.pad_token_id else -100 for label in target]
# print(labels)

origin_labels = [0, 203, 262, 264, 22, 265, 5850, 989, 1912, 1150, 319, 437, 95, 46, 348, 781, 13768, 1519, 1677, 291, 7452, 3160, 5698, 412, 2659, 1756, 7747, 10865, 11657, 388, 264, 29, 2183, 265, 225, 8516, 11657, 203, 311, 225, 203, 64, 619, 95, 1638, 656, 654, 15, 77, 64, 520, 64, 676, 64, 520, 325, 874, 82, 15, 21, 5137, 20, 64, 1919, 64, 676, 651, 64, 677, 700, 3531, 654, 15, 77, 64, 520, 64, 676, 64, 520, 325, 23518, 87, 3363, 677, 700, 10, 34, 20, 283, 64, 266, 64, 676, 651, 345, 534, 95, 1638, 97, 203, 312, 225, 203, 8877, 1756, 1674, 291, 12920, 225, 264, 74, 58, 265, 554, 7089, 1549, 554, 7089, 7268, 1747, 631, 554, 5181, 4505, 1586, 6576, 2759, 13768, 1519, 1677, 303, 4220, 1747, 631, 554, 5181, 4505, 1586, 264, 21, 265, 781, 9022, 1677, 291, 1225, 13768, 1519, 7430, 631, 2612, 924, 3425, 303, 9450, 1912, 1070, 203, 262, 25678, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]

assert labels == origin_labels